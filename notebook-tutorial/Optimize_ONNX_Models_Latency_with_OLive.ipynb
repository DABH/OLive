{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize ONNX Models Latency with OLive Python Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demos how to use OLive python package to optimize an ONNX model inference lantency performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Download OLive python package [here](https://olivewheels.blob.core.windows.net/repo/onnxruntime_olive-0.2.0-py3-none-any.whl)\n",
    "\n",
    "\n",
    "1. Install OLive with `pip install onnxruntime_olive-0.2.0-py3-none-any.whl`\n",
    "\n",
    "\n",
    "2. Install ONNX Runtime with \n",
    "\n",
    "    `pip install --extra-index-url https://olivewheels.azureedge.net/test onnxruntime_openvino_dnnl==1.9.0` for cpu\n",
    "    \n",
    "    or \n",
    "    \n",
    "    `pip install --extra-index-url https://olivewheels.azureedge.net/test onnxruntime_gpu_tensorrt==1.9.0` for gpu\n",
    "\n",
    "\n",
    "3. Download example model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "optimization_example = 'optimization_example'\n",
    "if not os.path.isdir(optimization_example):\n",
    "    os.mkdir(optimization_example)\n",
    "\n",
    "model_url = \"https://olivemodels.blob.core.windows.net/models/optimization/TFBertForQuestionAnswering.onnx\"\n",
    "model_response = urllib.request.urlretrieve(model_url, optimization_example + \"/TFBertForQuestionAnswering.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Optimize ONNX Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurations for OLive Model Optimization includes:\n",
    "\n",
    "| Configuration  | Detail  | Example |\n",
    "|:--|:--|:--|\n",
    "| **model_path** | model path for optimization | \"PytorchBertSquad.onnx\" |\n",
    "| **openmp_enabled** | whether the onnxruntime package is built with OpenMP | False |\n",
    "| **result_path** | result directory for OLive optimization | \"olive_opt_result\" |\n",
    "| **inputs_spec** | dict of inputâ€™s names and shapes | {\"attention_mask\": [1, 7], \"input_ids\": [1, 7], \"token_type_ids\": [1, 7]} |\n",
    "| **output_names** | output names for onnxruntime session inference | \"scores\" |\n",
    "| **providers_list** | providers used for perftuning | [\"cpu\", \"dnnl\"] |\n",
    "| **trt_fp16_enabled** | whether enable fp16 mode for TensorRT | True |\n",
    "| **quantization_enabled** | whether enable the quantization or not | True |\n",
    "| **transformer_enabled** | whether enable transformer optimization | True |\n",
    "| **transformer_args** | onnxruntime transformer optimizer args | \"--model_type bert\" |\n",
    "| **sample_input_data_path** | path to sample_input_data.npz | \"sample_input_data.npz\" |\n",
    "| **concurrency_num** | tuning process concurrency number | 2 |\n",
    "| **kmp_affinity** | bind OpenMP* threads to physical processing units | [\"respect,none\"] |\n",
    "| **omp_max_active_levels** | maximum number of nested active parallel regions | [\"1\"] |\n",
    "| **inter_thread_num_list** | list of inter thread number for perftuning | [1,2,4] |\n",
    "| **intra_thread_num_list** | list of intra thread number for perftuning | [1,2,4] |\n",
    "| **execution_mode_list** | list of execution mode for perftuning | [\"parallel\", \"sequential\"] |\n",
    "| **ort_opt_level_list** | onnxruntime optimization level | [\"all\"] |\n",
    "| **omp_wait_policy_list** | list of OpenMP wait policy for perftuning | [\"active\"] |\n",
    "| **warmup_num** | warmup times for latency measurement | 20 |\n",
    "| **test_num** | repeat test times for latency measurement | 200 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from olive.optimization_config import OptimizationConfig\n",
    "from olive.optimize import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_config = OptimizationConfig(\n",
    "    model_path = os.path.join(optimization_example, \"TFBertForQuestionAnswering.onnx\"),\n",
    "    result_path = \"olive_opt_latency_result\",\n",
    "    inputs_spec = {\"attention_mask\": [1, 7], \"input_ids\": [1, 7], \"token_type_ids\": [1, 7]},\n",
    "    providers_list = [\"cpu\", \"dnnl\"],\n",
    "    inter_thread_num_list = [1,2,4],\n",
    "    execution_mode_list = [\"parallel\", \"sequential\"],\n",
    "    warmup_num = 10,\n",
    "    test_num = 40)\n",
    "\n",
    "result = optimize(opt_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9a5dcffd1f681f7d477d1d70f92079f370ae67071dda428793bf72fce572af8a"
  },
  "kernelspec": {
   "display_name": "mlperf_36",
   "language": "python",
   "name": "mlperf_36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
